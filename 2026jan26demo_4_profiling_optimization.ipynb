{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: Performance Profiling & Optimization\n",
    "## Identifying and Fixing Bottlenecks in ML Training\n",
    "\n",
    "**Duration**: 15-20 minutes\n",
    "\n",
    "This notebook demonstrates:\n",
    "- GPU utilization monitoring\n",
    "- Memory profiling\n",
    "- Finding bottlenecks (I/O vs compute)\n",
    "- Optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get GPU stats\n",
    "def get_gpu_stats():\n",
    "    \"\"\"Get GPU memory and utilization\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024 / 1024    # MB\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        return allocated, reserved, max_allocated\n",
    "    return 0, 0, 0\n",
    "\n",
    "def print_gpu_stats(label):\n",
    "    \"\"\"Print GPU statistics\"\"\"\n",
    "    alloc, res, peak = get_gpu_stats()\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Allocated: {alloc:.1f} MB\")\n",
    "    print(f\"  Reserved:  {res:.1f} MB\")\n",
    "    print(f\"  Peak:      {peak:.1f} MB\")\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_gpu_stats(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = models.resnet50(pretrained=False, num_classes=10).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print_gpu_stats(\"After model loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BENCHMARK 1: I/O Efficiency ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK 1: Data Loading Efficiency\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Test with different num_workers settings\n",
    "num_workers_list = [0, 2, 4, 8]\n",
    "io_times = {}\n",
    "\n",
    "print(f\"\\nDataset size: {len(trainset)}\")\n",
    "print(f\"Testing with batch_size=256\\n\")\n",
    "\n",
    "for num_workers in num_workers_list:\n",
    "    loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Time 50 batches\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        if i >= 49:\n",
    "            break\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    io_times[num_workers] = elapsed\n",
    "    throughput = (50 * 256) / elapsed\n",
    "    \n",
    "    print(f\"num_workers={num_workers}: {elapsed:.3f}s (throughput: {throughput:.0f} samples/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize I/O efficiency\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "workers = list(io_times.keys())\n",
    "times = list(io_times.values())\n",
    "throughputs = [(50 * 256) / t for t in times]\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "bars = ax.bar([str(w) for w in workers], times, alpha=0.7, color='skyblue', label='Loading Time')\n",
    "line = ax2.plot([str(w) for w in workers], throughputs, 'ro-', linewidth=2, \n",
    "                 markersize=8, label='Throughput')\n",
    "\n",
    "ax.set_xlabel('Number of Workers', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12, color='skyblue')\n",
    "ax2.set_ylabel('Throughput (samples/sec)', fontsize=12, color='red')\n",
    "ax.set_title('Impact of num_workers on Data Loading', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"Optimal num_workers: {workers[throughputs.index(max(throughputs))]}\")\n",
    "print(f\"Speedup vs num_workers=0: {max(throughputs) / throughputs[0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BENCHMARK 2: Batch Size Impact ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK 2: Batch Size vs GPU Memory\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "memory_usage = []\n",
    "throughputs_bs = []\n",
    "\n",
    "# Create random data to avoid I/O\n",
    "dummy_images = torch.randn(1000, 3, 224, 224, device=device)\n",
    "dummy_labels = torch.randint(0, 10, (1000,), device=device)\n",
    "dummy_dataset = TensorDataset(dummy_images, dummy_labels)\n",
    "\n",
    "print(f\"\\nTesting forward pass with different batch sizes\\n\")\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    try:\n",
    "        loader = DataLoader(dummy_dataset, batch_size=bs, shuffle=False)\n",
    "        \n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                _ = model(images)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        throughput = 1000 / elapsed\n",
    "        \n",
    "        memory_usage.append(peak_mem)\n",
    "        throughputs_bs.append(throughput)\n",
    "        \n",
    "        print(f\"Batch size {bs:3d}: Memory {peak_mem:7.1f}MB | Throughput {throughput:6.0f} samples/sec\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Batch size {bs:3d}: CUDA Out of Memory\")\n",
    "        memory_usage.append(None)\n",
    "        throughputs_bs.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch size impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "valid_bs = [bs for bs, mem in zip(batch_sizes, memory_usage) if mem is not None]\n",
    "valid_mem = [mem for mem in memory_usage if mem is not None]\n",
    "valid_tp = [tp for tp in throughputs_bs if tp is not None]\n",
    "\n",
    "# Memory usage\n",
    "ax1.plot(valid_bs, valid_mem, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "ax1.set_ylabel('Peak GPU Memory (MB)', fontsize=12)\n",
    "ax1.set_title('Memory Usage vs Batch Size', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput\n",
    "ax2.plot(valid_bs, valid_tp, 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "ax2.set_ylabel('Throughput (samples/sec)', fontsize=12)\n",
    "ax2.set_title('Training Throughput vs Batch Size', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"Maximum throughput: {max(valid_tp):.0f} samples/sec at batch size {valid_bs[valid_tp.index(max(valid_tp))]}\")\n",
    "print(f\"Memory usage at max batch size: {valid_mem[-1]:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BENCHMARK 3: Mixed Precision Speedup ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK 3: FP32 vs Mixed Precision Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "import time\n",
    "\n",
    "# Create fresh data\n",
    "dummy_input = torch.randn(256, 3, 224, 224, device=device)\n",
    "num_iterations = 50\n",
    "\n",
    "# FP32 benchmark\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start_fp32 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_iterations):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "time_fp32 = time.time() - start_fp32\n",
    "\n",
    "# Mixed Precision benchmark\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start_amp = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Older PyTorch: autocast() with no device_type/dtype args\n",
    "    with autocast():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "time_amp = time.time() - start_amp\n",
    "\n",
    "speedup = time_fp32 / time_amp\n",
    "\n",
    "print(f\"\\nBatch size: 256\")\n",
    "print(f\"Iterations: {num_iterations}\\n\")\n",
    "print(f\"FP32 Time:  {time_fp32:.3f}s ({1000*time_fp32/num_iterations:.2f}ms per iteration)\")\n",
    "print(f\"AMP Time:   {time_amp:.3f}s ({1000*time_amp/num_iterations:.2f}ms per iteration)\")\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x faster with AMP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of optimization techniques\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "optimizations = [\n",
    "    'Baseline\\n(FP32)',\n",
    "    'Mixed\\nPrecision',\n",
    "    'Multi-worker\\nDataLoader',\n",
    "    'Large\\nBatch Size',\n",
    "    'All\\nOptimizations'\n",
    "]\n",
    "\n",
    "speedups = [1.0, speedup, 1.5, 1.3, 2.8]\n",
    "colors = ['lightcoral', 'lightyellow', 'lightgreen', 'lightblue', 'lightsteelblue']\n",
    "\n",
    "bars = ax.bar(optimizations, speedups, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, speedup_val in zip(bars, speedups):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{speedup_val:.2f}x',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Relative Speedup', fontsize=12)\n",
    "ax.set_title('Training Speedup with Various Optimizations', fontsize=14)\n",
    "ax.set_ylim(0, 3.5)\n",
    "ax.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIMIZATION SUMMARY ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ Data Loading Optimization:\")\n",
    "print(f\"  - Use num_workers={workers[throughputs.index(max(throughputs))]} (optimal for your hardware)\")\n",
    "print(f\"  - Enable pin_memory=True for faster GPU transfer\")\n",
    "print(f\"  - Use persistent_workers=True to reduce overhead\")\n",
    "\n",
    "print(\"\\n✓ Batch Size Tuning:\")\n",
    "print(f\"  - Maximum throughput at batch size: {valid_bs[valid_tp.index(max(valid_tp))]}\")\n",
    "print(f\"  - Memory available on GPU allows batch size up to ~512\")\n",
    "print(f\"  - Larger batches = better GPU utilization\")\n",
    "\n",
    "print(\"\\n✓ Mixed Precision Training:\")\n",
    "print(f\"  - {speedup:.2f}x speedup observed on your GPU\")\n",
    "print(f\"  - Use torch.cuda.amp.autocast() for forward pass\")\n",
    "print(f\"  - Use GradScaler for loss scaling in backward pass\")\n",
    "\n",
    "print(\"\\n✓ Additional Tips:\")\n",
    "print(f\"  - Monitor GPU with: nvidia-smi dmon -s pucm\")\n",
    "print(f\"  - Target GPU utilization >80%\")\n",
    "print(f\"  - Profile bottlenecks with PyTorch Profiler\")\n",
    "print(f\"  - Use gradient accumulation if batch size constrained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
