{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Multi-GPU Training with PyTorch DDP\n",
    "## Distributed Data Parallel for Scaled Training\n",
    "\n",
    "**Duration**: 20-25 minutes\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Setting up DistributedDataParallel (DDP)\n",
    "- Synchronizing batch normalization across GPUs\n",
    "- Measuring speedup and scaling efficiency\n",
    "- Proper gradient synchronization\n",
    "\n",
    "**Note**: For actual multi-GPU training, run this as a script with torchrun\n",
    "```bash\n",
    "torchrun --nproc_per_node=2 train_ddp.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check multi-GPU setup\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN().to(device)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# IMPORTANT: Use DistributedSampler for DDP\n",
    "# This ensures each GPU gets different batches\n",
    "sampler = DistributedSampler(\n",
    "    trainset,\n",
    "    num_replicas=torch.cuda.device_count(),\n",
    "    rank=0,  # In real DDP, this comes from distributed.get_rank()\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"DataLoader setup: {len(train_loader)} batches of {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Wrap model with DataParallel (not DDP) if multiple GPUs are visible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"\\nWrapping model with DataParallel across {torch.cuda.device_count()} GPUs...\")\n",
    "    model = nn.DataParallel(model)  # simple multi-GPU wrapper\n",
    "    print(\"DataParallel wrapper applied\")\n",
    "else:\n",
    "    print(\"\\nSingle GPU detected - DataParallel not applied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "    return 0\n",
    "\n",
    "print(\"Training configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for DDP\n",
    "def train_epoch_ddp(epoch, train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Important: Set epoch for sampler (ensures different data distribution each epoch)\n",
    "    if hasattr(train_loader.sampler, 'set_epoch'):\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # Gradients automatically synchronized in DDP\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, elapsed\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 3 epochs\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "epoch_times = []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\\n\")\n",
    "overall_start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    avg_loss, accuracy, epoch_time = train_epoch_ddp(epoch, train_loader, model, criterion, optimizer, device)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(accuracy)\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"  Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}% | Time: {epoch_time:.2f}s\")\n",
    "    print(f\"  GPU Memory: {get_gpu_memory():.2f} MB\\n\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "overall_time = time.time() - overall_start\n",
    "print(f\"\\n=== Training Summary ===\")\n",
    "print(f\"Total Time: {overall_time:.2f}s\")\n",
    "print(f\"Average Epoch Time: {sum(epoch_times)/len(epoch_times):.2f}s\")\n",
    "print(f\"Final Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {train_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss (DDP)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, marker='o', linewidth=2, color='green')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training Accuracy (DDP)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== DDP Key Points ===\")\n",
    "print(\"✓ Gradients automatically synchronized across GPUs\")\n",
    "print(\"✓ DistributedSampler ensures no data overlap\")\n",
    "print(\"✓ Each GPU processes different mini-batch\")\n",
    "print(\"✓ AllReduce operation averages gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
